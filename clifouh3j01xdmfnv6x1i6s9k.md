---
title: "æ¢è¨graph attentionæ©Ÿåˆ¶æœ‰æ•ˆæ€§â€Šâ€”â€ŠUnderstanding Attention and Generalization in Graph Neural Networks"
datePublished: Mon Mar 09 2020 15:08:16 GMT+0000 (Coordinated Universal Time)
cuid: clifouh3j01xdmfnv6x1i6s9k
slug: understanding-attention-and-generalization-in-graph-neural-networks
cover: https://cdn.hashnode.com/res/hashnode/image/upload/v1685778250122/306309b4-82e6-4fab-a5de-cbcae49dfb09.png
tags: ai, graph

---

paper link: [https://arxiv.org/abs/1905.02850](https://arxiv.org/abs/1905.02850)

ç™¼è¡¨æ–¼NIPS 2019

### å‰æ™¯æè¦

è‡ªå¾graph attention network (GAT)æå‡ºä»¥ä¾†(ä¸ç†Ÿæ‚‰GATçš„è©±å¯ä»¥åƒè€ƒä¸‹é¢å°å¤§æå®æ¯…æ•™æˆçš„åŠ©æ•™èª²å½±ç‰‡)ï¼Œattentionçš„æ¦‚å¿µè®Šæˆè¨“ç·´graphç›¸é—œçš„ç¥ç¶“ç¶²è·¯æ™‚ï¼Œä¸€å€‹å¾ˆpowerfulçš„å·¥å…·ï¼Œattentionç‰©ç†æ„ç¾©ä¸Šå°±æ˜¯æŸä¸€ç¯€é»å°å…¶ä»–ä¸åŒç¯€é»æ‡‰æœ‰ä¸åŒé—œæ³¨åº¦ï¼Œä¾‹å¦‚ä½ å–œæ„›çš„å¶åƒæˆ–å®¶äººèªªçš„è©±å°ä½ çš„å½±éŸ¿åŠ›ï¼Œç›¸æ¯”ä¸€å€‹æ™®é€šæœ‹å‹èªªçš„é«˜å‡ºè¨±å¤š

<iframe src="https://www.youtube.com/embed/eybCCtNKwzA?start=2071&feature=oembed&start=2071" width="640" height="480"></iframe>

ä½†æ˜¯ï¼Œè¦é‡åŒ–æˆ–å­¸ç¿’attentionçš„å¤§å°ä¸¦ä¸å®¹æ˜“ï¼Œæ›´ä¸ç”¨èªªå¯ä»¥äº‹å…ˆçŸ¥é“attentionçš„å€¼ï¼ˆä½¿supervisedå¼çš„è¨“ç·´æ–¹æ³•çª’ç¤™é›£è¡Œï¼‰ï¼Œç”šè‡³å¯èƒ½å¼„å·§æˆæ‹™ï¼Œä½¿modelçš„æ•ˆèƒ½ä¸‹é™ï¼ˆå› ç‚ºéåº¦é—œæ³¨äº†ä¸é‡è¦çš„nodeï¼‰ï¼Œä¾‹å¦‚ç•¶ä½ åªçœ‹ä¸­\*é›»è¦–å°ï¼ˆnodeï¼‰ï¼Œä½ å°æ–¼äº‹ç‰©çš„åˆ¤æ–·å°±æœƒæœ‰æ‰€åé —ï¼Œç”šè‡³è¦ºå¾—è‡ªå·±å¯ä»¥Fa da tsai ğŸ’°

> å› æ­¤ï¼Œæœ¬ç¯‡paperè—‰ç”±ä¸åŒå¯¦é©—ä¾†æ¢è¨graph attentionæ©Ÿåˆ¶æœ‰æ•ˆæ€§ï¼Œä¸¦æå‡ºä¸€å€‹weakly-supervisedçš„è¨“ç·´æ–¹æ³•ä¾†è¿‘ä¼¼supervisedçš„çµæœï¼Œåœ¨real datasetä¸Šä¹Ÿå¾—åˆ°äº†æ€§èƒ½æå‡

### æœ¬æ–‡é–‹å§‹

åœ¨graph neural networks (GNN)ä¸­ï¼Œattentionå¯ä»¥è¢«å®šç¾©åœ¨edgeä¸Šï¼ˆi.e. edge weightï¼‰ï¼Œä¹Ÿå¯ä»¥åœ¨nodeä¸Šï¼ˆi.e. node weightï¼‰ï¼Œæœ¬æ–‡çš„åˆ†æä¸»è¦focusåœ¨node weightä¸Š

> Attention meets pooling in graph neural networks

> é¦–å…ˆï¼Œä½œè€…å°‡attentionçš„æ¦‚å¿µèå…¥poolingï¼Œç”¨attentionä¾†åšgraph pooling

å€Ÿé‘’Top-k poolingï¼Œå°æ–¼é‚£äº›ä¸é‡è¦çš„nodeï¼ˆweightä½æ–¼æŸthresholdï¼Œpaperå…§å¯¦é©—çš„å–å€¼ç¯„åœ from 0.0001 to 0.1ï¼‰ï¼Œä¸æ˜¯çµ¦ä»–å€‘ä¸€å€‹è¼ƒä½çš„weightï¼Œè€Œæ˜¯ç›´æ¥dropä»–å€‘ï¼Œå¼·è¿«æ¨¡å‹æ›´focusåœ¨é‚£äº›å‰©ä¸‹çš„é‡è¦ç¯€é»

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1685778211544/af5f1297-b575-4d7a-a962-774268963a94.png align="left")

æœ¬æ–‡æå‡ºçš„attention-based poolingçš„æ–¹å¼

Top-k poolingä¸æ˜¯uniformly sampledï¼Œæ‰€ä»¥é€™æ–¹æ³•çš„å¥½è™•æ˜¯å¯ä»¥selectåˆ°ä¸åŒçš„å±€éƒ¨ç‰¹å¾µé»ï¼Œä½†å¾ˆæ˜é¡¯çš„ï¼Œé€™ç¨®ç›´æ¥ä¸Ÿæ‰ç¯€é»çš„æ–¹æ³•æœƒç ´å£graph structureï¼Œç”šè‡³ç”¢ç”Ÿisolated nodesï¼Œåªæ˜¯åœ¨æœ¬æ–‡çš„å¯¦é©—ä¸­ï¼Œä¸€å€‹é»çš„å‘¨åœç¯€é»é€šå¸¸æœƒæœ‰ç›¸ä¼¼å¤§å°çš„attentionï¼Œæ›å¥è©±èªªå°±æ˜¯ä½ è·Ÿä½ æœ‹å‹å€‘æœƒåŒæ™‚è¢«é¸ä¸Šæˆ–è½é¸ï¼Œå› æ­¤isolated nodesçš„å•é¡Œæ–¼æœ¬æ–‡ä¸­ä¸¦æ²’é€™éº¼åš´é‡

ç•¶attention layeræ‡‰ç”¨åœ¨inputå±¤æ™‚ï¼Œå…·æœ‰å¾ˆå¥½çš„interpretabilityï¼ˆsince the network makes a decision only based on pooled nodesï¼‰ï¼Œè€Œæœ¬æ–‡ä¹Ÿæ˜¯é€™éº¼è¨­è¨ˆçš„ï¼Œæ¦‚å¿µå¦‚ä¸‹åœ–

åŸåœ–ç¶“étrainable attention-based pooling layerèƒå–å‡ºé‡è¦çš„ç¯€é»ï¼Œå…¶é¤˜æ¨æ£„

> How to train an attention-based pooling layer

æ³•1: trainä¸€å€‹projection vector **pï¼ˆåœ¨å¾Œé¢çš„è¨è«–æœƒçŸ¥é“åœ¨unsupervisedæ™‚pçš„åˆå§‹åŒ–å¾ˆé‡è¦ï¼›supervisedå‰‡æ²’å·®ï¼‰**, then

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1685778213704/d83658b8-a447-4957-b5d6-31bed50a582d.png align="left")

Xç‚ºnode featureÂ map

æ³•2: trainä¸€å€‹GNN

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1685778215727/ebc41c5c-af5a-4a64-b0a6-d27f8d366a59.png align="left")

A is the adjacency matrix of a graph, Xç‚ºnode featureÂ map

ä¸¦ç¶“ésoftmaxä¾†provides more interpretable results and encourages sparse outputs

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1685778217546/84bb2f26-c0e0-4139-87b0-85bb4e458f49.png align="left")

ç‚ºäº†åœ¨supervised or weakly-supervisedçš„æƒ…æ³éƒ½èƒ½è¨ˆç®—lossï¼Œä½œè€…ä½¿ç”¨äº†KLæ•£åº¦ï¼ˆè¡¡é‡å…©å€‹åˆ†ä½ˆçš„å·®ç•°ï¼Œåœ¨æ¦‚ç‡è«–èˆ‡çµ±è¨ˆä¸­ï¼Œæˆ‘å€‘ç¶“å¸¸æœƒå°‡ä¸€å€‹è¤‡é›œçš„åˆ†ä½ˆç”¨ä¸€å€‹ç°¡å–®çš„è¿‘ä¼¼åˆ†ä½ˆä¾†ä»£æ›¿ã€‚KL æ•£åº¦å¯ä»¥å¹«åŠ©æˆ‘å€‘æ¸¬é‡åœ¨é¸æ“‡ä¸€å€‹è¿‘ä¼¼åˆ†ä½ˆæ™‚ä¸Ÿå¤±çš„ä¿¡æ¯é‡ã€‚ï¼‰ï¼Œå› æ­¤trainingçš„total lossç‚ºï¼ˆå¾Œé¢æœƒè©³ç´°è§£é‡‹ï¼‰

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1685778219814/d13d199f-baca-425f-ad8f-e49d9e155101.png align="left")

åƒæ•¸æ„ç¾©å¯åƒè€ƒpaper sectionÂ 3.3

> Another model

åœ¨æœ¬æ–‡çš„ä¸€äº›å¯¦é©—ä¸­ï¼ŒGraph Convolutional Networks (GCN) å’Œ Graph Isomorphism Networks (GINï¼Œé‹ç”¨äº†sum aggregatorèšåˆå®Œç¯€é»ç‰¹å¾µå¾ŒåŠ ä¸Šæ›´å¤šçš„fully-connected layersï¼Œåœ¨åˆ¤åˆ¥graph structuresæ™‚æœ‰æ›´å¥½çš„æ€§èƒ½ï¼‰çš„è¡¨ç¾ä¸¦ä¸å¥½ï¼Œå› æ­¤æœ¬æ–‡ä¹ŸçµåˆGINå’ŒChebyNetï¼Œæå‡ºä¸€å€‹æ›´strongerçš„model - ChebyGIN

### å¯¦é©—éƒ¨ä»½

> Datasetsèªªæ˜

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1685778222977/0b6488cd-0439-4ac6-b3de-215134041d11.png align="left")

1\. Color counting task (ä¸Šåœ–aï¼‰

æŠŠåŸåœ–å„ç¯€é»è‘—è‰²ï¼Œè¨ˆç®—æœ‰å¹¾å€‹ç¶ è‰²ç¯€é»ï¼Œå› ç‚ºç¯„ä¾‹æœ‰å…©å€‹ç¶ è‰²ï¼Œæ‰€ä»¥å…¶Ground truth attentionå„ç‚º1/2 = 0.5ï¼Œåœ¨é€™å€‹taskä¸­ï¼Œ the graph structure is unimportant and edges of graphs act like a medium to exchange node features

2\. Counting the number of trianglesï¼ˆä¸Šåœ–bï¼‰

æ•¸å‡ºåŸåœ–ä¸­æœ‰å¹¾å€‹ä¸‰è§’å½¢ï¼Œé€™ä»»å‹™å„å€‹nodeçš„Ground truth attentionå¯ä»¥é€éä¸‹å¼ä¾†è¨ˆç®—ï¼Œæ¯”å¦‚ç¯„ä¾‹åœ–å…±æœ‰2å€‹ä¸‰è§’å½¢ï¼Œç”±åœ–å¯çŸ¥å››å€‹è·Ÿä¸‰è§’å½¢æœ‰é—œä¿‚çš„nodeçš„Tiåˆ†åˆ¥ç‚º1 2 2 1ï¼Œ1+2+2+1 = 6ï¼Œå› æ­¤attentionç‚º0.2çš„ç¯€é»æ˜¯ç”±1/6å››æ¨äº”å…¥å¾—ä¾†ï¼›0.3æ˜¯ç”±2/6å››æ¨äº”å…¥å¾—ä¾†

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1685778225517/2c9ebc0c-f5d1-42d5-a423-c0e46483c37d.png align="left")

3\. MNIST-75SPï¼ˆä¸Šåœ–cï¼‰

ç‚ºäº†æ¸¬è©¦GNNåœ¨irregular gridsä¸Šçš„èƒ½åŠ›ï¼Œä½œè€…æ ¹æ“šå‰äººçš„è«–æ–‡æ”¹è‰¯äº†MNIST datasetï¼Œå°‡imageåœ¨é¿å…éºå¤±essential class-specific informationçš„å‰æä¸‹ï¼Œä»¥a small set of superpixels ä¾†å»ºå‡ºä¸€å€‹graphä»£è¡¨åŸæœ¬çš„imageï¼Œé€™å€‹graphçš„node featuresç‚ºaverage intensity value to all pixels within a superpixelå’Œè¶…åƒç´ çš„é‡å¿ƒåº§æ¨™ï¼›edgesç‚ºè¶…åƒç´ ä¸­å¿ƒä¹‹é–“çš„ç©ºé–“è·é›¢ï¼Œ Ground truth attentionè¢«å®šç¾©ç‚º

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1685778228579/fc226f78-8425-4da8-8b42-688c25c3f745.png align="left")

ä½œè€…å°æ¯å¼µimageå–N â‰¤ 75å€‹superpixels, å› æ­¤é€™datasetè¢«ç¨±ç‚ºMNIST-75SP

4\. Molecule and social datasets - more practical cases

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1685778230898/7f54a491-bb0f-410c-84e8-5aa40c6868ea.png align="left")

åœ¨practical casesä¸­ï¼Œä½œè€…ç”¨äº†graph classificationä»»å‹™çš„benchmark datasetsï¼Œå¦‚è›‹ç™½è³ªçµæ§‹dataset: PROTEINS and D&Dï¼›ä»¥åŠscientific collaboration dataset: COLLABï¼Œå¯¦é©—ä¸­ï¼Œä½œè€…æƒ³æ¢è¨attention-based modelåœ¨inductiveä»»å‹™çš„èƒ½åŠ›ï¼Œå› æ­¤æ ¹æ“šç¯€é»æ•¸é‡åˆ†å‰²graph datasetï¼Œä¾‹å¦‚PROTEINSä¸­ï¼Œwe train on graphs with N â‰¤ 25 nodes and test on graphs with 6 â‰¤ N â‰¤ 620 nodes

ä¸‹åœ–ç‚ºå‰ä¸‰å€‹ä»»å‹™çš„train&test data examples

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1685778233766/331bbba1-c2ff-42b6-aced-b66f1f72b83a.png align="left")

> Generalization to larger and noisy graphs

attentionçš„ä¸€å€‹èƒ½åŠ›æ˜¯generalize to unseen, potentially more complex and/or noisy, inputsï¼Œè—‰ç”±æ¸›å°‘é—œæ³¨é€™äº›çˆ›nodesä¾†æ›´å¥½çš„è¨“ç·´ï¼Œå› æ­¤é€™ç¯‡paperåœ¨test caseä¸ŠæŠŠåŸæœ¬çš„dataåšäº›è®Šæ›ï¼Œä¾‹å¦‚ä¸Šåœ–colorä»»å‹™çš„TEST-LARGEå¢åŠ ä¸æ˜¯ç­”æ¡ˆçš„ç¯€é»ï¼ˆéç¶ è‰²ï¼‰æˆ–TEST-LARGECå¢åŠ ä¸åŒunseené¡è‰²çš„nodesï¼Œé€™äº›ä»¥ä¸‹çµ±ç¨±ç‚ºé›œè¨Šï¼Œä½œè€…æƒ³è—‰ç”±with and without attentionçš„GNNä¾†æ¢è¨the limits of GNNs with attentionï¼Œå’Œattentionåœ¨å“ªäº›ä»»å‹™æœƒworkï¼Œå“ªäº›ä»»å‹™åè€Œæœ‰å®³ï¼ˆfocusä¸é‡è¦çš„nodes or dropé‡è¦çš„nodesï¼‰

> Loss function and training

COLORS and TRIANGLESä»»å‹™ä¸­è¦minimize the regression loss (MSE)ï¼Œå…¶ä»–ä»»å‹™å‰‡minimize cross entropy (CE)ï¼Œå¾Œé …çš„KLæ•£åº¦è¦è¡¡é‡çš„åˆ†ä½ˆæ˜¯ground truth attentionå’Œpredicted attention

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1685778236014/1628c10a-176f-491b-8363-6f21261b563b.png align="left")

åƒæ•¸æ„ç¾©å¯åƒè€ƒpaper sectionÂ 3.3

> Weakly-supervised attention supervision

ç¾å¯¦æƒ…æ³ä¸­ï¼Œé å…ˆçŸ¥é“ground truth attentionå¹¾ä¹æ˜¯ä¸å¯èƒ½çš„ï¼Œå› æ­¤æœ¬æ–‡æå‡ºäº†ä¸€ç¨®å¼±ç›£ç£çš„è¨“ç·´æ–¹æ³•ä¾†ä¼°ç®—attentionï¼Œè—‰æ­¤optimizeä¸Šé¢çš„total loss

é¦–å…ˆæˆ‘å€‘æƒ³trainä¸€å€‹model Aï¼Œä½†æˆ‘å€‘æ²’æœ‰ground truth attentionï¼Œæ‰€ä»¥æˆ‘å€‘å¯ä»¥å…ˆtrainä¸€å€‹å’Œmodel Aé•·çš„ä¸€æ¨£çš„model Bï¼Œåªæ˜¯model Bæ²’æœ‰attention/poolingæ©Ÿåˆ¶ï¼Œåªæœ‰æœ€å¾Œçš„global pooling (e.g. readoutï¼‰ä¾†åšåˆ†é¡æ©Ÿç‡è¼¸å‡ºï¼Œå› æ­¤model Båªæœ‰optimize MSE or CEçš„lossï¼Œè¨“ç·´å¥½ä¹‹å¾Œï¼Œæˆ‘å€‘å¯ä»¥ç”¨ä»¥ä¸‹å¼å­ä¾†è—‰ç”±model Bä¼°ç®—attentionï¼š

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1685778238051/228a61b4-66e5-4b59-9142-bc92a45df9ac.png align="left")

yæ˜¯åŸåœ–åˆ†é¡çš„predictionï¼›yiæ˜¯åŸåœ–å»é™¤node içš„åˆ†é¡prediction

æ¦‚å¿µå°±æ˜¯æˆ‘è—‰ç”±è§€å¯Ÿæœ‰node iè·Ÿæ²’æœ‰node iï¼Œæ©Ÿç‡åˆ†ä½ˆçš„çµæœæœƒå·®å¤šå°‘ï¼Œå¦‚æœæ²’ä»€éº¼å·®ï¼Œä»£è¡¨æœ‰æ²’æœ‰é€™nodeæ ¹æœ¬ä¸é‡è¦ï¼Œå…¶attentionè‡ªç„¶å°±ä½ï¼Œå› æ­¤è—‰ç”±ä¸Šå¼&model Bï¼Œå¯ä»¥å¾—åˆ°ä¼°ç®—çš„attentionçµ¦model Aè¨“ç·´ç”¨

### å€‹äººèªç‚ºé€™æ–¹æ³•åªèƒ½çŸ¥é“nodeå°æ•´é«”graphçš„å½±éŸ¿ï¼Œä¸èƒ½çŸ¥é“æ˜¯å¥½çš„å½±éŸ¿é‚„æ˜¯å£çš„å½±éŸ¿ï¼Œç›¸ä¿¡é€™ä¹Ÿæ˜¯ä¹‹å¾Œå¯ä»¥æ”¹é€²çš„éƒ¨åˆ†

### çµæœåˆ†æ

![](https://cdn-images-1.medium.com/max/800/1*inaggtq98_6xqqJKWu5MKQ.png align="left")

å¾ä¸Šè¡¨å¯çœ‹å‡ºï¼Œé‹ç”¨attention over nodeså¯ä»¥è®“model generalize to more complex or noisy graphs at test timeï¼Œå°¤å…¶å¯ä»¥å¾COLORSçš„çµæœçœ‹å‡ºï¼ŒGINé‹ç”¨global poolingå’Œé‹ç”¨attention-based poolingçš„çµæœï¼Œå·®äº†60å¹¾%

å¾å¯¦é©—çµæœä¹Ÿå¯ä»¥çœ‹å‡ºæœ¬æ–‡æå‡ºçš„weakly-supervisedæ–¹æ³•shows performance, robustness and relatively low variation (i.e. sensitivity to initialization) similar to supervised models and much better than the unsupervised model

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1685778242362/dc6dd55b-fe09-4ddd-91f1-404929e679e2.png align="left")

åœ¨çœŸå¯¦datasetä¸­ï¼Œsupervised attentionæ˜¯ä¸å¯è¡Œçš„ï¼Œè€Œæœ¬æ–‡æå‡ºçš„å¼±ç›£ç£è¨“ç·´æ–¹æ³•ä¹Ÿå–å¾—äº†æ¯”ç„¡ç›£ç£æ›´å¥½çš„çµæœ

> What are the factors influencing the performance of GNNs with attention?

1. **initialization of the attention model (i.e. vector p or GNN)**
    
2. strength of the main GNN model (i.e. the model that actually performs classification)
    
3. other hyper-parameters of the attention and GNN models.
    

ç¬¬ä¸€é»æ˜¯æœ€é‡è¦çš„ï¼Œæˆ‘å€‘å¯ä»¥å¾ä¸‹é¢å…©å¼µåœ–æ¢è¨

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1685778245452/237f9e1d-1f55-4df0-91e6-b0aaff92ac8b.png align="left")

ç„¡ç›£ç£attentionçš„çµæœï¼ˆå®¹æ˜“å¡åœ¨local optimalï¼‰

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1685778248052/01132bd7-890d-4da9-a1d4-557f66d3c0b9.png align="left")

ç›£ç£attentionçš„çµæœ

> Why is the variance of some results so high?

Caused by the initialization of other trainable parameters of a GNN, but we show that once the attention model is perfect, other parameters can recover from a bad initialization leading to better results. The opposite, however, is not true: we never observed the recovery of a model with poorly initialized attention

### çµè«–

attentionæ©Ÿåˆ¶åœ¨GNNæ˜¯éå¸¸powerfulçš„ï¼Œä½†å‰ææ˜¯æœ‰å€‹å¥½çš„**initialization of the attention modelï¼ˆstill an open issueï¼‰ï¼Œåä¹‹å‰‡æœƒæå®³æ¨¡å‹**ï¼Œpaperä¹Ÿé€éå¯¦é©—è­‰æ˜ï¼Œattention can make GNNs more robust to larger and noisy graphsï¼Œæœ¬æ–‡æå‡ºçš„attention-based weakly-supervisedè¨“ç·´æ–¹æ³•ä¹Ÿèƒ½åœ¨real datasetä¸­work

And thatâ€™s a wrap! Enjoy. ğŸ†

ğŸ‘